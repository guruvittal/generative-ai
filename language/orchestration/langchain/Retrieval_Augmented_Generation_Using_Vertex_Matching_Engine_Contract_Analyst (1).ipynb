{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj7R48-ZXmTC"
      },
      "source": [
        "# Retrieval Augmented Generation - Procurement Contract Analyst -  Palm2 & LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2D5-yDE6Z7NX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPG8eRQCcNQ3"
      },
      "source": [
        "## Installation & Authentication\n",
        "\n",
        "**Install google-cloud-aiplatform & langchain**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdsE3DEJcM35"
      },
      "outputs": [],
      "source": [
        "# Install langchain and related libraries\n",
        "!pip install langchain unstructured\n",
        "\n",
        "# Install Vertex AI LLM SDK\n",
        "! pip install google-cloud-aiplatform==1.25.0\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptwMB9pqcniz"
      },
      "source": [
        "**Authenticate**\n",
        "\n",
        "Within colab, a simple user authentication is adequate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HP80SWi0rIBL"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth as google_auth\n",
        "    google_auth.authenticate_user()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Get Libraries & Classes"
      ],
      "metadata": {
        "id": "oEX1X9z0VF1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LangChain Libraries"
      ],
      "metadata": {
        "id": "uV6PX1_hT5Jg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import VertexAIEmbeddings\n",
        "from langchain.vectorstores import MatchingEngine\n",
        "\n",
        "\"\"\"Vertex Matching Engine implementation of the vector store.\"\"\"\n",
        "from __future__ import annotations\n",
        "\n",
        "import logging\n",
        "import uuid\n",
        "from typing import Any, Iterable, List, Optional, Type\n",
        "\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.embeddings import TensorflowHubEmbeddings\n",
        "from langchain.embeddings.base import Embeddings\n",
        "from langchain.embeddings import VertexAIEmbeddings\n",
        "from langchain.vectorstores.base import VectorStore\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import json\n"
      ],
      "metadata": {
        "id": "8smAwjNcT9hn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lcArn48r6pZ"
      },
      "source": [
        "### Vertex Libraries, Classes & Helper Functions\n",
        "**Reference Libraries**\n",
        "\n",
        "In this section, we will identify all the library classes that will be referenced in the code.\n",
        "\n",
        "**Classes Defined**\n",
        "*   _VertexCommon\n",
        "*   VertexLLM\n",
        "*   VertexEmbeddings\n",
        "\n",
        "\n",
        "**Functions Defined**\n",
        "\n",
        "*   rate_limit\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ax6hlCt7YbXx",
        "outputId": "f1116fab-09c9-47a8-c292-8b0e58bece1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vertex AI SDK version: 1.25.0\n"
          ]
        }
      ],
      "source": [
        "# Using Vertex AI\n",
        "import vertexai\n",
        "from google.cloud import aiplatform\n",
        "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")\n",
        "\n",
        "# Using Google Cloud Storage Directory loader from langchain\n",
        "from langchain.document_loaders import GCSDirectoryLoader\n",
        "\n",
        "import time\n",
        "\n",
        "from pydantic import BaseModel, Extra, root_validator\n",
        "from typing import Any, Mapping, Optional, List, Dict\n",
        "from langchain.llms.base import LLM\n",
        "from langchain.embeddings.base import Embeddings\n",
        "\n",
        "\n",
        "from google.cloud import storage\n",
        "from google.cloud.aiplatform import MatchingEngineIndex, MatchingEngineIndexEndpoint\n",
        "from google.cloud import aiplatform_v1\n",
        "from google.oauth2.service_account import Credentials\n",
        "import google.auth\n",
        "import google.auth.transport.requests\n",
        "\n",
        "\n",
        "\n",
        "class _VertexCommon(BaseModel):\n",
        "    \"\"\"Wrapper around Vertex AI large language models.\n",
        "\n",
        "    To use, you should have the\n",
        "    ``google.cloud.aiplatform.private_preview.language_models`` python package\n",
        "    installed.\n",
        "    \"\"\"\n",
        "    client: Any = None #: :meta private:\n",
        "    model_name: str = \"text-bison@001\"\n",
        "    \"\"\"Model name to use.\"\"\"\n",
        "\n",
        "    temperature: float = 0.2\n",
        "    \"\"\"What sampling temperature to use.\"\"\"\n",
        "\n",
        "    top_p: int = 0.8\n",
        "    \"\"\"Total probability mass of tokens to consider at each step.\"\"\"\n",
        "\n",
        "    top_k: int = 40\n",
        "    \"\"\"The number of highest probability tokens to keep for top-k filtering.\"\"\"\n",
        "\n",
        "    max_output_tokens: int = 200\n",
        "    \"\"\"The maximum number of tokens to generate in the completion.\"\"\"\n",
        "\n",
        "    @property\n",
        "    def _default_params(self) -> Mapping[str, Any]:\n",
        "        \"\"\"Get the default parameters for calling Vertex AI API.\"\"\"\n",
        "        return {\n",
        "            \"temperature\": self.temperature,\n",
        "            \"top_p\": self.top_p,\n",
        "            \"top_k\": self.top_k,\n",
        "            \"max_output_tokens\": self.max_output_tokens\n",
        "        }\n",
        "\n",
        "    def _predict(self, prompt: str, stop: Optional[List[str]]) -> str:\n",
        "        res = self.client.predict(prompt, **self._default_params)\n",
        "        return self._enforce_stop_words(res.text, stop)\n",
        "\n",
        "    def _enforce_stop_words(self, text: str, stop: Optional[List[str]]) -> str:\n",
        "        if stop:\n",
        "            return enforce_stop_tokens(text, stop)\n",
        "        return text\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        \"\"\"Return type of llm.\"\"\"\n",
        "        return \"vertex_ai\"\n",
        "\n",
        "class VertexLLM(_VertexCommon, LLM):\n",
        "    model_name: str = \"text-bison@001\"\n",
        "\n",
        "    @root_validator()\n",
        "    def validate_environment(cls, values: Dict) -> Dict:\n",
        "        \"\"\"Validate that the python package exists in environment.\"\"\"\n",
        "        try:\n",
        "            from vertexai.preview.language_models import TextGenerationModel\n",
        "        except ImportError:\n",
        "            raise ValueError(\n",
        "                \"Could not import Vertex AI LLM python package. \"\n",
        "            )\n",
        "\n",
        "        try:\n",
        "            values[\"client\"] = TextGenerationModel.from_pretrained(values[\"model_name\"])\n",
        "        except AttributeError:\n",
        "            raise ValueError(\n",
        "                \"Could not set Vertex Text Model client.\"\n",
        "            )\n",
        "\n",
        "        return values\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        \"\"\"Call out to Vertex AI's create endpoint.\n",
        "\n",
        "        Args:\n",
        "            prompt: The prompt to pass into the model.\n",
        "\n",
        "        Returns:\n",
        "            The string generated by the model.\n",
        "        \"\"\"\n",
        "        return self._predict(prompt, stop)\n",
        "\n",
        "\n",
        "\n",
        "def rate_limit(max_per_minute):\n",
        "  period = 60 / max_per_minute\n",
        "  print('Waiting')\n",
        "  while True:\n",
        "    before = time.time()\n",
        "    yield\n",
        "    after = time.time()\n",
        "    elapsed = after - before\n",
        "    sleep_time = max(0, period - elapsed)\n",
        "    if sleep_time > 0:\n",
        "      print('.', end='')\n",
        "      time.sleep(sleep_time)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yHf4ipxYyfy"
      },
      "source": [
        "## Initiatlize Vertex AI\n",
        "\n",
        "**We will need a project id and location where the Vertex compute and embedding will be hosted**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EZe8iS2CY2E8"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"argolis-project-340214\"  # @param {type:\"string\"}\n",
        "\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "# Initialize Vertex AI SDK\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0htdnYAHonv"
      },
      "source": [
        "## Build the Matching Engine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuwJlBNwIy0y"
      },
      "source": [
        "\n",
        "\n",
        "###*Define Parameters & Create Bucket for Embeddings*\n",
        "Set the locations of the documents, embeddings, index and dimensions for the embedding vector*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPvIUvqJg_fi",
        "outputId": "95062ff3-0877-4086-9781-977fc55fa1c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+ gsutil mb -p argolis-project-340214 -l us-central1 gs://argolis-project-340214-me-bucket\n",
            "Creating gs://argolis-project-340214-me-bucket/...\n",
            "ServiceException: 409 A Cloud Storage bucket named 'argolis-project-340214-me-bucket' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
          ]
        }
      ],
      "source": [
        "ME_BUCKET = \"matching_engine_bucket\"\n",
        "ME_REGION = \"us-central1\"\n",
        "ME_INDEX_NAME = f\"{PROJECT_ID}-me-index\"  # @param {type:\"string\"}\n",
        "ME_EMBEDDING_DIR = f\"{PROJECT_ID}-me-bucket\"  # @param {type:\"string\"}\n",
        "ME_DIMENSIONS = 768  # @param {type:\"integer\"} when using Vertex PaLM Embedding\n",
        "\n",
        "! set -x && gsutil mb -p $PROJECT_ID -l us-central1 gs://$ME_EMBEDDING_DIR\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*Define Matching Engine Class & Utilities Class*"
      ],
      "metadata": {
        "id": "E4AXBRDVIADR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vY9n_20-i0oO"
      },
      "outputs": [],
      "source": [
        "\n",
        "logger = logging.getLogger()\n",
        "\n",
        "\n",
        "class MatchingEngine(VectorStore):\n",
        "    \"\"\"Vertex Matching Engine implementation of the vector store.\n",
        "\n",
        "    While the embeddings are stored in the Matching Engine, the embedded\n",
        "    documents will be stored in GCS.\n",
        "\n",
        "    An existing Index and corresponding Endpoint are preconditions for\n",
        "    using this module.\n",
        "\n",
        "    See usage in docs/modules/indexes/vectorstores/examples/matchingengine.ipynb\n",
        "\n",
        "    Note that this implementation is mostly meant for reading if you are\n",
        "    planning to do a real time implementation. While reading is a real time\n",
        "    operation, updating the index takes close to one hour.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        project_id: str,\n",
        "        region: str,\n",
        "        index: MatchingEngineIndex,\n",
        "        endpoint: MatchingEngineIndexEndpoint,\n",
        "        embedding: Embeddings,\n",
        "        gcs_client: storage.Client,\n",
        "        index_client: aiplatform_v1.IndexServiceClient,\n",
        "        index_endpoint_client: aiplatform_v1.IndexEndpointServiceClient,\n",
        "        gcs_bucket_name: str,\n",
        "        credentials: Credentials = None,\n",
        "    ):\n",
        "        \"\"\"Vertex Matching Engine implementation of the vector store.\n",
        "\n",
        "        While the embeddings are stored in the Matching Engine, the embedded\n",
        "        documents will be stored in GCS.\n",
        "\n",
        "        An existing Index and corresponding Endpoint are preconditions for\n",
        "        using this module.\n",
        "\n",
        "        See usage in\n",
        "        docs/modules/indexes/vectorstores/examples/matchingengine.ipynb.\n",
        "\n",
        "        Note that this implementation is mostly meant for reading if you are\n",
        "        planning to do a real time implementation. While reading is a real time\n",
        "        operation, updating the index takes close to one hour.\n",
        "\n",
        "        Attributes:\n",
        "            project_id: The GCS project id.\n",
        "            index: The created index class. See\n",
        "            ~:func:`MatchingEngine.from_components`.\n",
        "            endpoint: The created endpoint class. See\n",
        "            ~:func:`MatchingEngine.from_components`.\n",
        "            embedding: A :class:`Embeddings` that will be used for\n",
        "            embedding the text sent. If none is sent, then the\n",
        "            multilingual Tensorflow Universal Sentence Encoder will be used.\n",
        "            gcs_client: The Google Cloud Storage client.\n",
        "            credentials (Optional): Created GCP credentials.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self._validate_google_libraries_installation()\n",
        "\n",
        "        self.project_id = project_id\n",
        "        self.region = region\n",
        "        self.index = index\n",
        "        self.endpoint = endpoint\n",
        "        self.embedding = embedding\n",
        "        self.gcs_client = gcs_client\n",
        "        self.index_client = index_client\n",
        "        self.index_endpoint_client = index_endpoint_client\n",
        "        self.gcs_client = gcs_client\n",
        "        self.credentials = credentials\n",
        "        self.gcs_bucket_name = gcs_bucket_name\n",
        "\n",
        "    def _validate_google_libraries_installation(self) -> None:\n",
        "        \"\"\"Validates that Google libraries that are needed are installed.\"\"\"\n",
        "        try:\n",
        "            from google.cloud import aiplatform, storage  # noqa: F401\n",
        "            from google.oauth2 import service_account  # noqa: F401\n",
        "        except ImportError:\n",
        "            raise ImportError(\n",
        "                \"You must run `pip install --upgrade \"\n",
        "                \"google-cloud-aiplatform google-cloud-storage`\"\n",
        "                \"to use the MatchingEngine Vectorstore.\"\n",
        "            )\n",
        "\n",
        "    def add_texts(\n",
        "        self,\n",
        "        texts: Iterable[str],\n",
        "        metadatas: Optional[List[dict]] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> List[str]:\n",
        "        \"\"\"Run more texts through the embeddings and add to the vectorstore.\n",
        "\n",
        "        Args:\n",
        "            texts: Iterable of strings to add to the vectorstore.\n",
        "            metadatas: Optional list of metadatas associated with the texts.\n",
        "            kwargs: vectorstore specific parameters.\n",
        "\n",
        "        Returns:\n",
        "            List of ids from adding the texts into the vectorstore.\n",
        "        \"\"\"\n",
        "        logger.debug(\"Embedding documents.\")\n",
        "        embeddings = self.embedding.embed_documents(list(texts))\n",
        "        insert_datapoints_payload = []\n",
        "        ids = []\n",
        "\n",
        "        # Streaming index update\n",
        "        for idx, (embedding, text, metadata) in enumerate(\n",
        "            zip(embeddings, texts, metadatas)\n",
        "        ):\n",
        "            id = uuid.uuid4()\n",
        "            ids.append(id)\n",
        "            self._upload_to_gcs(text, f\"documents/{id}\")\n",
        "            metadatas[idx]\n",
        "            insert_datapoints_payload.append(\n",
        "                aiplatform_v1.IndexDatapoint(\n",
        "                    datapoint_id=str(id),\n",
        "                    feature_vector=embedding,\n",
        "                    restricts=metadata if metadata else [],\n",
        "                )\n",
        "            )\n",
        "            if idx % 100 == 0:\n",
        "                upsert_request = aiplatform_v1.UpsertDatapointsRequest(\n",
        "                    index=self.index.name, datapoints=insert_datapoints_payload\n",
        "                )\n",
        "                response = self.index_client.upsert_datapoints(request=upsert_request)\n",
        "                insert_datapoints_payload = []\n",
        "        if len(insert_datapoints_payload) > 0:\n",
        "            upsert_request = aiplatform_v1.UpsertDatapointsRequest(\n",
        "                index=self.index.name, datapoints=insert_datapoints_payload\n",
        "            )\n",
        "            _ = self.index_client.upsert_datapoints(request=upsert_request)\n",
        "\n",
        "        logger.debug(\"Updated index with new configuration.\")\n",
        "        logger.info(f\"Indexed {len(ids)} documents to Matching Engine.\")\n",
        "\n",
        "        return ids\n",
        "\n",
        "    def _upload_to_gcs(self, data: str, gcs_location: str) -> None:\n",
        "        \"\"\"Uploads data to gcs_location.\n",
        "\n",
        "        Args:\n",
        "            data: The data that will be stored.\n",
        "            gcs_location: The location where the data will be stored.\n",
        "        \"\"\"\n",
        "        bucket = self.gcs_client.get_bucket(self.gcs_bucket_name)\n",
        "        blob = bucket.blob(gcs_location)\n",
        "        blob.upload_from_string(data)\n",
        "\n",
        "    def get_matches(\n",
        "        self,\n",
        "        embeddings: List[str],\n",
        "        n_matches: int,\n",
        "        index_endpoint: MatchingEngineIndexEndpoint,\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        get matches from matching engine given a vector query\n",
        "        Uses public endpoint\n",
        "\n",
        "        \"\"\"\n",
        "        import requests\n",
        "        import json\n",
        "\n",
        "        request_data = {\n",
        "            \"deployed_index_id\": index_endpoint.deployed_indexes[0].id,\n",
        "            \"return_full_datapoint\": True,\n",
        "            \"queries\": [\n",
        "                {\n",
        "                    \"datapoint\": {\"datapoint_id\": f\"{i}\", \"feature_vector\": emb},\n",
        "                    \"neighbor_count\": n_matches,\n",
        "                }\n",
        "                for i, emb in enumerate(embeddings)\n",
        "            ],\n",
        "        }\n",
        "\n",
        "        endpoint_address = self.endpoint.public_endpoint_domain_name\n",
        "        rpc_address = f\"https://{endpoint_address}/v1beta1/{index_endpoint.resource_name}:findNeighbors\"\n",
        "        endpoint_json_data = json.dumps(request_data)\n",
        "\n",
        "        logger.debug(f\"Querying Matching Engine Index Endpoint {rpc_address}\")\n",
        "\n",
        "        request = google.auth.transport.requests.Request()\n",
        "        self.credentials.refresh(request)\n",
        "        header = {\"Authorization\": \"Bearer \" + self.credentials.token}\n",
        "\n",
        "        return requests.post(rpc_address, data=endpoint_json_data, headers=header)\n",
        "\n",
        "    def similarity_search(\n",
        "        self, query: str, k: int = 4, search_distance: float = 0.65, **kwargs: Any\n",
        "    ) -> List[Document]:\n",
        "        \"\"\"Return docs most similar to query.\n",
        "\n",
        "        Args:\n",
        "            query: The string that will be used to search for similar documents.\n",
        "            k: The amount of neighbors that will be retrieved.\n",
        "            search_distance: filter search results by  search distance by adding a threshold value\n",
        "\n",
        "        Returns:\n",
        "            A list of k matching documents.\n",
        "        \"\"\"\n",
        "\n",
        "        logger.debug(f\"Embedding query {query}.\")\n",
        "        embedding_query = self.embedding.embed_documents([query])\n",
        "        deployed_index_id = self._get_index_id()\n",
        "        logger.debug(f\"Deployed Index ID = {deployed_index_id}\")\n",
        "\n",
        "        # TO-DO: Pending query sdk integration\n",
        "        # response = self.endpoint.match(\n",
        "        #     deployed_index_id=self._get_index_id(),\n",
        "        #     queries=embedding_query,\n",
        "        #     num_neighbors=k,\n",
        "        # )\n",
        "\n",
        "        response = self.get_matches(embedding_query, k, self.endpoint)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            response = response.json()[\"nearestNeighbors\"]\n",
        "        else:\n",
        "            raise Exception(f\"Failed to query index {str(response)}\")\n",
        "\n",
        "        if len(response) == 0:\n",
        "            return []\n",
        "\n",
        "        logger.debug(f\"Found {len(response)} matches for the query {query}.\")\n",
        "\n",
        "        results = []\n",
        "\n",
        "        # I'm only getting the first one because queries receives an array\n",
        "        # and the similarity_search method only recevies one query. This\n",
        "        # means that the match method will always return an array with only\n",
        "        # one element.\n",
        "        for doc in response[0][\"neighbors\"]:\n",
        "            page_content = self._download_from_gcs(\n",
        "                f\"documents/{doc['datapoint']['datapointId']}\"\n",
        "            )\n",
        "            metadata = {}\n",
        "            if \"restricts\" in doc[\"datapoint\"]:\n",
        "                metadata = {\n",
        "                    item[\"namespace\"]: item[\"allowList\"][0]\n",
        "                    for item in doc[\"datapoint\"][\"restricts\"]\n",
        "                }\n",
        "            if \"distance\" in doc:\n",
        "                metadata[\"score\"] = doc[\"distance\"]\n",
        "                if doc[\"distance\"] >= search_distance:\n",
        "                    results.append(\n",
        "                        Document(page_content=page_content, metadata=metadata)\n",
        "                    )\n",
        "            else:\n",
        "                results.append(Document(page_content=page_content, metadata=metadata))\n",
        "\n",
        "        logger.debug(\"Downloaded documents for query.\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _get_index_id(self) -> str:\n",
        "        \"\"\"Gets the correct index id for the endpoint.\n",
        "\n",
        "        Returns:\n",
        "            The index id if found (which should be found) or throws\n",
        "            ValueError otherwise.\n",
        "        \"\"\"\n",
        "        for index in self.endpoint.deployed_indexes:\n",
        "            if index.index == self.index.name:\n",
        "                return index.id\n",
        "\n",
        "        raise ValueError(\n",
        "            f\"No index with id {self.index.name} \"\n",
        "            f\"deployed on enpoint \"\n",
        "            f\"{self.endpoint.display_name}.\"\n",
        "        )\n",
        "\n",
        "    def _download_from_gcs(self, gcs_location: str) -> str:\n",
        "        \"\"\"Downloads from GCS in text format.\n",
        "\n",
        "        Args:\n",
        "            gcs_location: The location where the file is located.\n",
        "\n",
        "        Returns:\n",
        "            The string contents of the file.\n",
        "        \"\"\"\n",
        "        bucket = self.gcs_client.get_bucket(self.gcs_bucket_name)\n",
        "        try:\n",
        "            blob = bucket.blob(gcs_location)\n",
        "            return blob.download_as_string()\n",
        "        except Exception:\n",
        "            return \"\"\n",
        "\n",
        "    @classmethod\n",
        "    def from_texts(\n",
        "        cls: Type[\"MatchingEngine\"],\n",
        "        texts: List[str],\n",
        "        embedding: Embeddings,\n",
        "        metadatas: Optional[List[dict]] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> \"MatchingEngine\":\n",
        "        \"\"\"Use from components instead.\"\"\"\n",
        "        raise NotImplementedError(\n",
        "            \"This method is not implemented. Instead, you should initialize the class\"\n",
        "            \" with `MatchingEngine.from_components(...)` and then call \"\n",
        "            \"`from_texts`\"\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def from_documents(\n",
        "        cls: Type[\"MatchingEngine\"],\n",
        "        documents: List[str],\n",
        "        embedding: Embeddings,\n",
        "        metadatas: Optional[List[dict]] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> \"MatchingEngine\":\n",
        "        \"\"\"Use from components instead.\"\"\"\n",
        "        raise NotImplementedError(\n",
        "            \"This method is not implemented. Instead, you should initialize the class\"\n",
        "            \" with `MatchingEngine.from_components(...)` and then call \"\n",
        "            \"`from_documents`\"\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def from_components(\n",
        "        cls: Type[\"MatchingEngine\"],\n",
        "        project_id: str,\n",
        "        region: str,\n",
        "        gcs_bucket_name: str,\n",
        "        index_id: str,\n",
        "        endpoint_id: str,\n",
        "        credentials_path: Optional[str] = None,\n",
        "        embedding: Optional[Embeddings] = None,\n",
        "    ) -> \"MatchingEngine\":\n",
        "        \"\"\"Takes the object creation out of the constructor.\n",
        "\n",
        "        Args:\n",
        "            project_id: The GCP project id.\n",
        "            region: The default location making the API calls. It must have\n",
        "            the same location as the GCS bucket and must be regional.\n",
        "            gcs_bucket_name: The location where the vectors will be stored in\n",
        "            order for the index to be created.\n",
        "            index_id: The id of the created index.\n",
        "            endpoint_id: The id of the created endpoint.\n",
        "            credentials_path: (Optional) The path of the Google credentials on\n",
        "            the local file system.\n",
        "            embedding: The :class:`Embeddings` that will be used for\n",
        "            embedding the texts.\n",
        "\n",
        "        Returns:\n",
        "            A configured MatchingEngine with the texts added to the index.\n",
        "        \"\"\"\n",
        "        gcs_bucket_name = cls._validate_gcs_bucket(gcs_bucket_name)\n",
        "\n",
        "        # Set credentials\n",
        "        if credentials_path:\n",
        "            credentials = cls._create_credentials_from_file(credentials_path)\n",
        "        else:\n",
        "            credentials, _ = google.auth.default()\n",
        "            request = google.auth.transport.requests.Request()\n",
        "            credentials.refresh(request)\n",
        "\n",
        "        index = cls._create_index_by_id(index_id, project_id, region, credentials)\n",
        "        endpoint = cls._create_endpoint_by_id(\n",
        "            endpoint_id, project_id, region, credentials\n",
        "        )\n",
        "\n",
        "        gcs_client = cls._get_gcs_client(credentials, project_id)\n",
        "        index_client = cls._get_index_client(project_id, region, credentials)\n",
        "        index_endpoint_client = cls._get_index_endpoint_client(\n",
        "            project_id, region, credentials\n",
        "        )\n",
        "        cls._init_aiplatform(project_id, region, gcs_bucket_name, credentials)\n",
        "\n",
        "        return cls(\n",
        "            project_id=project_id,\n",
        "            region=region,\n",
        "            index=index,\n",
        "            endpoint=endpoint,\n",
        "            embedding=embedding or cls._get_default_embeddings(),\n",
        "            gcs_client=gcs_client,\n",
        "            index_client=index_client,\n",
        "            index_endpoint_client=index_endpoint_client,\n",
        "            credentials=credentials,\n",
        "            gcs_bucket_name=gcs_bucket_name,\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def _validate_gcs_bucket(cls, gcs_bucket_name: str) -> str:\n",
        "        \"\"\"Validates the gcs_bucket_name as a bucket name.\n",
        "\n",
        "        Args:\n",
        "              gcs_bucket_name: The received bucket uri.\n",
        "\n",
        "        Returns:\n",
        "              A valid gcs_bucket_name or throws ValueError if full path is\n",
        "              provided.\n",
        "        \"\"\"\n",
        "        gcs_bucket_name = gcs_bucket_name.replace(\"gs://\", \"\")\n",
        "        if \"/\" in gcs_bucket_name:\n",
        "            raise ValueError(\n",
        "                f\"The argument gcs_bucket_name should only be \"\n",
        "                f\"the bucket name. Received {gcs_bucket_name}\"\n",
        "            )\n",
        "        return gcs_bucket_name\n",
        "\n",
        "    @classmethod\n",
        "    def _create_credentials_from_file(\n",
        "        cls, json_credentials_path: Optional[str]\n",
        "    ) -> Optional[Credentials]:\n",
        "        \"\"\"Creates credentials for GCP.\n",
        "\n",
        "        Args:\n",
        "             json_credentials_path: The path on the file system where the\n",
        "             credentials are stored.\n",
        "\n",
        "         Returns:\n",
        "             An optional of Credentials or None, in which case the default\n",
        "             will be used.\n",
        "        \"\"\"\n",
        "\n",
        "        from google.oauth2 import service_account\n",
        "\n",
        "        credentials = None\n",
        "        if json_credentials_path is not None:\n",
        "            credentials = service_account.Credentials.from_service_account_file(\n",
        "                json_credentials_path\n",
        "            )\n",
        "\n",
        "        return credentials\n",
        "\n",
        "    @classmethod\n",
        "    def _create_index_by_id(\n",
        "        cls, index_id: str, project_id: str, region: str, credentials: \"Credentials\"\n",
        "    ) -> MatchingEngineIndex:\n",
        "        \"\"\"Creates a MatchingEngineIndex object by id.\n",
        "\n",
        "        Args:\n",
        "            index_id: The created index id.\n",
        "\n",
        "        Returns:\n",
        "            A configured MatchingEngineIndex.\n",
        "        \"\"\"\n",
        "\n",
        "        from google.cloud import aiplatform_v1\n",
        "\n",
        "        logger.debug(f\"Creating matching engine index with id {index_id}.\")\n",
        "        index_client = cls._get_index_client(project_id, region, credentials)\n",
        "        request = aiplatform_v1.GetIndexRequest(name=index_id)\n",
        "        return index_client.get_index(request=request)\n",
        "\n",
        "    @classmethod\n",
        "    def _create_endpoint_by_id(\n",
        "        cls, endpoint_id: str, project_id: str, region: str, credentials: \"Credentials\"\n",
        "    ) -> MatchingEngineIndexEndpoint:\n",
        "        \"\"\"Creates a MatchingEngineIndexEndpoint object by id.\n",
        "\n",
        "        Args:\n",
        "            endpoint_id: The created endpoint id.\n",
        "\n",
        "        Returns:\n",
        "            A configured MatchingEngineIndexEndpoint.\n",
        "            :param project_id:\n",
        "            :param region:\n",
        "            :param credentials:\n",
        "        \"\"\"\n",
        "\n",
        "        from google.cloud import aiplatform\n",
        "\n",
        "        logger.debug(f\"Creating endpoint with id {endpoint_id}.\")\n",
        "        return aiplatform.MatchingEngineIndexEndpoint(\n",
        "            index_endpoint_name=endpoint_id,\n",
        "            project=project_id,\n",
        "            location=region,\n",
        "            credentials=credentials,\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def _get_gcs_client(\n",
        "        cls, credentials: \"Credentials\", project_id: str\n",
        "    ) -> \"storage.Client\":\n",
        "        \"\"\"Lazily creates a GCS client.\n",
        "\n",
        "        Returns:\n",
        "            A configured GCS client.\n",
        "        \"\"\"\n",
        "\n",
        "        from google.cloud import storage\n",
        "\n",
        "        return storage.Client(credentials=credentials, project=project_id)\n",
        "\n",
        "    @classmethod\n",
        "    def _get_index_client(\n",
        "        cls, project_id: str, region: str, credentials: \"Credentials\"\n",
        "    ) -> \"storage.Client\":\n",
        "        \"\"\"Lazily creates a Matching Engine Index client.\n",
        "\n",
        "        Returns:\n",
        "            A configured Matching Engine Index client.\n",
        "        \"\"\"\n",
        "\n",
        "        from google.cloud import aiplatform_v1\n",
        "\n",
        "        # PARENT = f\"projects/{project_id}/locations/{region}\"\n",
        "        ENDPOINT = f\"{region}-aiplatform.googleapis.com\"\n",
        "        return aiplatform_v1.IndexServiceClient(\n",
        "            client_options=dict(api_endpoint=ENDPOINT), credentials=credentials\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def _get_index_endpoint_client(\n",
        "        cls, project_id: str, region: str, credentials: \"Credentials\"\n",
        "    ) -> \"storage.Client\":\n",
        "        \"\"\"Lazily creates a Matching Engine Index Endpoint client.\n",
        "\n",
        "        Returns:\n",
        "            A configured Matching Engine Index Endpoint client.\n",
        "        \"\"\"\n",
        "\n",
        "        from google.cloud import aiplatform_v1\n",
        "\n",
        "        # PARENT = f\"projects/{project_id}/locations/{region}\"\n",
        "        ENDPOINT = f\"{region}-aiplatform.googleapis.com\"\n",
        "        return aiplatform_v1.IndexEndpointServiceClient(\n",
        "            client_options=dict(api_endpoint=ENDPOINT), credentials=credentials\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def _init_aiplatform(\n",
        "        cls,\n",
        "        project_id: str,\n",
        "        region: str,\n",
        "        gcs_bucket_name: str,\n",
        "        credentials: \"Credentials\",\n",
        "    ) -> None:\n",
        "        \"\"\"Configures the aiplatform library.\n",
        "\n",
        "        Args:\n",
        "            project_id: The GCP project id.\n",
        "            region: The default location making the API calls. It must have\n",
        "            the same location as the GCS bucket and must be regional.\n",
        "            gcs_bucket_name: GCS staging location.\n",
        "            credentials: The GCS Credentials object.\n",
        "        \"\"\"\n",
        "\n",
        "        from google.cloud import aiplatform\n",
        "\n",
        "        logger.debug(\n",
        "            f\"Initializing AI Platform for project {project_id} on \"\n",
        "            f\"{region} and for {gcs_bucket_name}.\"\n",
        "        )\n",
        "        aiplatform.init(\n",
        "            project=project_id,\n",
        "            location=region,\n",
        "            staging_bucket=gcs_bucket_name,\n",
        "            credentials=credentials,\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def _get_default_embeddings(cls) -> TensorflowHubEmbeddings:\n",
        "        \"\"\"This function returns the default embedding.\"\"\"\n",
        "        return TensorflowHubEmbeddings()\n",
        "\n",
        "# Utility functions to create Index and deploy the index to an Endpoint\n",
        "from datetime import datetime\n",
        "import time\n",
        "import logging\n",
        "\n",
        "from google.cloud import aiplatform_v1 as aipv1\n",
        "from google.protobuf import struct_pb2\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger()\n",
        "\n",
        "\n",
        "class MatchingEngineUtils:\n",
        "    def __init__(self, project_id: str, region: str, index_name: str):\n",
        "        self.project_id = project_id\n",
        "        self.region = region\n",
        "        self.index_name = index_name\n",
        "        self.index_endpoint_name = f\"{self.index_name}-endpoint\"\n",
        "        self.PARENT = f\"projects/{self.project_id}/locations/{self.region}\"\n",
        "\n",
        "        ENDPOINT = f\"{self.region}-aiplatform.googleapis.com\"\n",
        "        # set index client\n",
        "        self.index_client = aipv1.IndexServiceClient(\n",
        "            client_options=dict(api_endpoint=ENDPOINT)\n",
        "        )\n",
        "        # set index endpoint client\n",
        "        self.index_endpoint_client = aipv1.IndexEndpointServiceClient(\n",
        "            client_options=dict(api_endpoint=ENDPOINT)\n",
        "        )\n",
        "\n",
        "    def get_index(self):\n",
        "        # Check if index exists\n",
        "        request = aipv1.ListIndexesRequest(parent=self.PARENT)\n",
        "        page_result = self.index_client.list_indexes(request=request)\n",
        "        indexes = [\n",
        "            response.name\n",
        "            for response in page_result\n",
        "            if response.display_name == self.index_name\n",
        "        ]\n",
        "\n",
        "        if len(indexes) == 0:\n",
        "            return None\n",
        "        else:\n",
        "            index_id = indexes[0]\n",
        "            request = aipv1.GetIndexRequest(name=index_id)\n",
        "            index = self.index_client.get_index(request=request)\n",
        "            return index\n",
        "\n",
        "    def get_index_endpoint(self):\n",
        "        # Check if index endpoint exists\n",
        "        request = aipv1.ListIndexEndpointsRequest(parent=self.PARENT)\n",
        "        page_result = self.index_endpoint_client.list_index_endpoints(request=request)\n",
        "        index_endpoints = [\n",
        "            response.name\n",
        "            for response in page_result\n",
        "            if response.display_name == self.index_endpoint_name\n",
        "        ]\n",
        "\n",
        "        if len(index_endpoints) == 0:\n",
        "            return None\n",
        "        else:\n",
        "            index_endpoint_id = index_endpoints[0]\n",
        "            request = aipv1.GetIndexEndpointRequest(name=index_endpoint_id)\n",
        "            index_endpoint = self.index_endpoint_client.get_index_endpoint(\n",
        "                request=request\n",
        "            )\n",
        "            return index_endpoint\n",
        "\n",
        "    def create_index(\n",
        "        self,\n",
        "        embedding_gcs_uri: str,\n",
        "        dimensions: int,\n",
        "        index_update_method: str = \"streaming\",\n",
        "        index_algorithm: str = \"tree-ah\",\n",
        "    ):\n",
        "        # Get index\n",
        "        index = self.get_index()\n",
        "        # Create index if does not exists\n",
        "        if index:\n",
        "            logger.info(f\"Index {self.index_name} already exists with id {index.name}\")\n",
        "        else:\n",
        "            logger.info(f\"Index {self.index_name} does not exists. Creating index ...\")\n",
        "\n",
        "            if index_update_method == \"streaming\":\n",
        "                index_update_method = aipv1.Index.IndexUpdateMethod.STREAM_UPDATE\n",
        "            else:\n",
        "                index_update_method = aipv1.Index.IndexUpdateMethod.BATCH_UPDATE\n",
        "\n",
        "            treeAhConfig = struct_pb2.Struct(\n",
        "                fields={\n",
        "                    \"leafNodeEmbeddingCount\": struct_pb2.Value(number_value=500),\n",
        "                    \"leafNodesToSearchPercent\": struct_pb2.Value(number_value=7),\n",
        "                }\n",
        "            )\n",
        "            if index_algorithm == \"treeah\":\n",
        "                algorithmConfig = struct_pb2.Struct(\n",
        "                    fields={\"treeAhConfig\": struct_pb2.Value(struct_value=treeAhConfig)}\n",
        "                )\n",
        "            else:\n",
        "                algorithmConfig = struct_pb2.Struct(\n",
        "                    fields={\n",
        "                        \"bruteForceConfig\": struct_pb2.Value(\n",
        "                            struct_value=struct_pb2.Struct()\n",
        "                        )\n",
        "                    }\n",
        "                )\n",
        "            config = struct_pb2.Struct(\n",
        "                fields={\n",
        "                    \"dimensions\": struct_pb2.Value(number_value=dimensions),\n",
        "                    \"approximateNeighborsCount\": struct_pb2.Value(number_value=150),\n",
        "                    \"distanceMeasureType\": struct_pb2.Value(\n",
        "                        string_value=\"DOT_PRODUCT_DISTANCE\"\n",
        "                    ),\n",
        "                    \"algorithmConfig\": struct_pb2.Value(struct_value=algorithmConfig),\n",
        "                    \"shardSize\": struct_pb2.Value(string_value=\"SHARD_SIZE_SMALL\"),\n",
        "                }\n",
        "            )\n",
        "            metadata = struct_pb2.Struct(\n",
        "                fields={\n",
        "                    \"config\": struct_pb2.Value(struct_value=config),\n",
        "                    \"contentsDeltaUri\": struct_pb2.Value(\n",
        "                        string_value=embedding_gcs_uri\n",
        "                    ),\n",
        "                }\n",
        "            )\n",
        "\n",
        "            index_request = {\n",
        "                \"display_name\": self.index_name,\n",
        "                \"description\": \"Index for LangChain demo\",\n",
        "                \"metadata\": struct_pb2.Value(struct_value=metadata),\n",
        "                \"index_update_method\": index_update_method,\n",
        "            }\n",
        "\n",
        "            r = self.index_client.create_index(parent=self.PARENT, index=index_request)\n",
        "            logger.info(\n",
        "                f\"Creating index with long running operation {r._operation.name}\"\n",
        "            )\n",
        "\n",
        "            # Poll the operation until it's done successfullly.\n",
        "            logging.info(\"Poll the operation to create index ...\")\n",
        "            while True:\n",
        "                if r.done():\n",
        "                    break\n",
        "                time.sleep(60)\n",
        "                print(\".\", end=\"\")\n",
        "\n",
        "            index = r.result()\n",
        "            logger.info(\n",
        "                f\"Index {self.index_name} created with resource name as {index.name}\"\n",
        "            )\n",
        "\n",
        "        return index\n",
        "\n",
        "    def deploy_index(\n",
        "        self,\n",
        "        machine_type: str = \"e2-standard-2\",\n",
        "        min_replica_count: int = 2,\n",
        "        max_replica_count: int = 10,\n",
        "        network: str = None,\n",
        "    ):\n",
        "        try:\n",
        "            # Get index if exists\n",
        "            index = self.get_index()\n",
        "            if not index:\n",
        "                raise Exception(\n",
        "                    f\"Index {self.index_name} does not exists. Please create index before deploying.\"\n",
        "                )\n",
        "\n",
        "            # Get index endpoint if exists\n",
        "            index_endpoint = self.get_index_endpoint()\n",
        "            # Create Index Endpoint if does not exists\n",
        "            if index_endpoint:\n",
        "                logger.info(\n",
        "                    f\"Index endpoint {self.index_endpoint_name} already exists with resource \"\n",
        "                    + f\"name as {index_endpoint.name} and endpoint domain name as \"\n",
        "                    + f\"{index_endpoint.public_endpoint_domain_name}\"\n",
        "                )\n",
        "            else:\n",
        "                logger.info(\n",
        "                    f\"Index endpoint {self.index_endpoint_name} does not exists. Creating index endpoint...\"\n",
        "                )\n",
        "                index_endpoint_request = {\"display_name\": self.index_endpoint_name}\n",
        "\n",
        "                if network:\n",
        "                    index_endpoint_request[\"network\"] = network\n",
        "                else:\n",
        "                    index_endpoint_request[\"public_endpoint_enabled\"] = True\n",
        "\n",
        "                r = self.index_endpoint_client.create_index_endpoint(\n",
        "                    parent=self.PARENT, index_endpoint=index_endpoint_request\n",
        "                )\n",
        "                logger.info(\n",
        "                    f\"Deploying index to endpoint with long running operation {r._operation.name}\"\n",
        "                )\n",
        "\n",
        "                logger.info(\"Poll the operation to create index endpoint ...\")\n",
        "                while True:\n",
        "                    if r.done():\n",
        "                        break\n",
        "                    time.sleep(60)\n",
        "                    print(\".\", end=\"\")\n",
        "\n",
        "                index_endpoint = r.result()\n",
        "                logger.info(\n",
        "                    f\"Index endpoint {self.index_endpoint_name} created with resource \"\n",
        "                    + f\"name as {index_endpoint.name} and endpoint domain name as \"\n",
        "                    + f\"{index_endpoint.public_endpoint_domain_name}\"\n",
        "                )\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to create index endpoint {self.index_endpoint_name}\")\n",
        "            raise e\n",
        "\n",
        "        # Deploy Index to endpoint\n",
        "        try:\n",
        "            # Check if index is already deployed to the endpoint\n",
        "            for d_index in index_endpoint.deployed_indexes:\n",
        "                if d_index.index == index.name:\n",
        "                    logger.info(\n",
        "                        f\"Skipping deploying Index. Index {self.index_name}\"\n",
        "                        + f\"already deployed with id {index.name} to the index endpoint {self.index_endpoint_name}\"\n",
        "                    )\n",
        "                    return index_endpoint\n",
        "\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "            deployed_index_id = f\"{self.index_name.replace('-', '_')}_{timestamp}\"\n",
        "            deploy_index = {\n",
        "                \"id\": deployed_index_id,\n",
        "                \"display_name\": deployed_index_id,\n",
        "                \"index\": index.name,\n",
        "                \"dedicated_resources\": {\n",
        "                    \"machine_spec\": {\n",
        "                        \"machine_type\": machine_type,\n",
        "                    },\n",
        "                    \"min_replica_count\": min_replica_count,\n",
        "                    \"max_replica_count\": max_replica_count,\n",
        "                },\n",
        "            }\n",
        "            logger.info(f\"Deploying index with request = {deploy_index}\")\n",
        "            r = self.index_endpoint_client.deploy_index(\n",
        "                index_endpoint=index_endpoint.name, deployed_index=deploy_index\n",
        "            )\n",
        "\n",
        "            # Poll the operation until it's done successfullly.\n",
        "            logger.info(\"Poll the operation to deploy index ...\")\n",
        "            while True:\n",
        "                if r.done():\n",
        "                    break\n",
        "                time.sleep(60)\n",
        "                print(\".\", end=\"\")\n",
        "\n",
        "            logger.info(\n",
        "                f\"Deployed index {self.index_name} to endpoint {self.index_endpoint_name}\"\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(\n",
        "                f\"Failed to deploy index {self.index_name} to the index endpoint {self.index_endpoint_name}\"\n",
        "            )\n",
        "            raise e\n",
        "\n",
        "        return index_endpoint\n",
        "\n",
        "    def get_index_and_endpoint(self):\n",
        "        # Get index id if exists\n",
        "        index = self.get_index()\n",
        "        index_id = index.name if index else \"\"\n",
        "\n",
        "        # Get index endpoint id if exists\n",
        "        index_endpoint = self.get_index_endpoint()\n",
        "        index_endpoint_id = index_endpoint.name if index_endpoint else \"\"\n",
        "\n",
        "        return index_id, index_endpoint_id\n",
        "\n",
        "    def delete_index(self):\n",
        "        # Check if index exists\n",
        "        index = self.get_index()\n",
        "\n",
        "        # create index if does not exists\n",
        "        if index:\n",
        "            # Delete index\n",
        "            index_id = index.name\n",
        "            logger.info(f\"Deleting Index {self.index_name} with id {index_id}\")\n",
        "            self.index_client.delete_index(name=index_id)\n",
        "        else:\n",
        "            raise Exception(\"Index {index_name} does not exists.\")\n",
        "\n",
        "    def delete_index_endpoint(self):\n",
        "        # Check if index endpoint exists\n",
        "        index_endpoint = self.get_index_endpoint()\n",
        "\n",
        "        # Create Index Endpoint if does not exists\n",
        "        if index_endpoint:\n",
        "            logger.info(\n",
        "                f\"Index endpoint {self.index_endpoint_name}  exists with resource \"\n",
        "                + f\"name as {index_endpoint.name} and endpoint domain name as \"\n",
        "                + f\"{index_endpoint.public_endpoint_domain_name}\"\n",
        "            )\n",
        "\n",
        "            index_endpoint_id = index_endpoint.name\n",
        "            index_endpoint = self.index_endpoint_client.get_index_endpoint(\n",
        "                name=index_endpoint_id\n",
        "            )\n",
        "            # Undeploy existing indexes\n",
        "            for d_index in index_endpoint.deployed_indexes:\n",
        "                logger.info(\n",
        "                    f\"Undeploying index with id {d_index.id} from Index endpoint {self.index_endpoint_name}\"\n",
        "                )\n",
        "                request = aipv1.UndeployIndexRequest(\n",
        "                    index_endpoint=index_endpoint_id, deployed_index_id=d_index.id\n",
        "                )\n",
        "                r = self.index_endpoint_client.undeploy_index(request=request)\n",
        "                response = r.result()\n",
        "                logger.info(response)\n",
        "\n",
        "            # Delete index endpoint\n",
        "            logger.info(\n",
        "                f\"Deleting Index endpoint {self.index_endpoint_name} with id {index_endpoint_id}\"\n",
        "            )\n",
        "            self.index_endpoint_client.delete_index_endpoint(name=index_endpoint_id)\n",
        "        else:\n",
        "            raise Exception(\n",
        "                f\"Index endpoint {self.index_endpoint_name} does not exists.\"\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*Initialize embedding directory with a null vector*"
      ],
      "metadata": {
        "id": "lkE0QpAEIo7A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9u0Bu56kWWm",
        "outputId": "2da09602-28d6-4fe0-9377-037d10dc06ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+ gsutil cp embeddings_0.json gs://argolis-project-340214-me-bucket/init_index/embeddings_0.json\n",
            "Copying file://embeddings_0.json [Content-Type=application/json]...\n",
            "/ [1 files][  3.8 KiB/  3.8 KiB]                                                \n",
            "Operation completed over 1 objects/3.8 KiB.                                      \n"
          ]
        }
      ],
      "source": [
        "\n",
        "# dummy embedding\n",
        "init_embedding = {\"id\": str(uuid.uuid4()), \"embedding\": list(np.zeros(ME_DIMENSIONS))}\n",
        "\n",
        "# dump embedding to a local file\n",
        "with open(\"embeddings_0.json\", \"w\") as f:\n",
        "    json.dump(init_embedding, f)\n",
        "\n",
        "# write embedding to Cloud Storage\n",
        "! set -x && gsutil cp embeddings_0.json gs://{ME_EMBEDDING_DIR}/init_index/embeddings_0.json"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*Create a Matching Engine Index and deploy to a endpoint*"
      ],
      "metadata": {
        "id": "soiuCisTJp9o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1rxq9YnWrw5",
        "outputId": "e3ab0edf-7a71-4fa3-f3f4-c1335cc1d2cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "projects/742458474659/locations/us-central1/indexes/6528535007873466368\n",
            "Index endpoint resource name: projects/742458474659/locations/us-central1/indexEndpoints/5965585054452154368\n",
            "Index endpoint public domain name: 1784211276.us-central1-742458474659.vdb.vertexai.goog\n",
            "Deployed indexes on the index endpoint:\n",
            "    argolis_project_340214_me_index_20231014181057\n"
          ]
        }
      ],
      "source": [
        "mengine = MatchingEngineUtils(PROJECT_ID, ME_REGION, ME_INDEX_NAME)\n",
        "\n",
        "index = mengine.create_index(\n",
        "    embedding_gcs_uri=f\"gs://{ME_EMBEDDING_DIR}/init_index\",\n",
        "    dimensions=ME_DIMENSIONS,\n",
        "    index_update_method=\"streaming\",\n",
        "    index_algorithm=\"tree-ah\",\n",
        ")\n",
        "if index:\n",
        "    print(index.name)\n",
        "\n",
        "index_endpoint = mengine.deploy_index()\n",
        "if index_endpoint:\n",
        "    print(f\"Index endpoint resource name: {index_endpoint.name}\")\n",
        "    print(\n",
        "        f\"Index endpoint public domain name: {index_endpoint.public_endpoint_domain_name}\"\n",
        "    )\n",
        "    print(\"Deployed indexes on the index endpoint:\")\n",
        "    for d in index_endpoint.deployed_indexes:\n",
        "        print(f\"    {d.id}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ME_INDEX_ID, ME_INDEX_ENDPOINT_ID = mengine.get_index_and_endpoint()\n",
        "print(f\"ME_INDEX_ID={ME_INDEX_ID}\")\n",
        "print(f\"ME_INDEX_ENDPOINT_ID={ME_INDEX_ENDPOINT_ID}\")\n",
        "print(f\"ME_INDEX_NAME={ME_INDEX_NAME}\")\n",
        "print(f\"ME_BUCKET={ME_BUCKET}\")\n",
        "print(f\"PROJECT_ID={PROJECT_ID}\")\n",
        "print(f\"ME_REGION={ME_REGION}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6RmKdSNwmLS",
        "outputId": "f2a052e2-da18-4d69-a8e9-512499882ca1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ME_INDEX_ID=projects/742458474659/locations/us-central1/indexes/6528535007873466368\n",
            "ME_INDEX_ENDPOINT_ID=projects/742458474659/locations/us-central1/indexEndpoints/5965585054452154368\n",
            "ME_INDEX_NAME=argolis-project-340214-me-index\n",
            "ME_BUCKET=matching_engine_bucket\n",
            "PROJECT_ID=argolis-project-340214\n",
            "ME_REGION=us-central1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*Create Embedding Engine & Build a Data Store*"
      ],
      "metadata": {
        "id": "JKWQSeChKYUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Text Embeddings model\n",
        "embedding = VertexAIEmbeddings()\n"
      ],
      "metadata": {
        "id": "cXNxt4cwyMes"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Matching Engine as Vector Store\n",
        "me = MatchingEngine.from_components(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=ME_REGION,\n",
        "    gcs_bucket_name=f'gs://{ME_BUCKET}',\n",
        "    embedding=embedding,\n",
        "    index_id=ME_INDEX_ID,\n",
        "    endpoint_id=ME_INDEX_ENDPOINT_ID)\n"
      ],
      "metadata": {
        "id": "UWSxHHO4YNZ3"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YozENqoAmm9"
      },
      "source": [
        "## Absorb documents, split them into chunks and capture metadata\n",
        "\n",
        "*Loading...*\n",
        "This takes some time: 1min++"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ME_BUCKET)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yqLac6INgt0",
        "outputId": "bc122cad-dd3c-4b42-e116-4ff4d6dfa5c3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "matching_engine_bucket\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Td4rD2MQtM1O",
        "outputId": "1fb1589f-bdb9-4496-fae7-02f3f42c1495"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "loader = GCSDirectoryLoader(project_name=PROJECT_ID, bucket=\"contractunderstandingatticusdataset\")\n",
        "contractdocs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzLAH66JfqwZ",
        "outputId": "0bc9ab77-76f1-4dfd-ad2a-50aaf12bb618"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of documents = 30\n",
            "{'source': 'gs://contractunderstandingatticusdataset/2ThemartComInc_19990826_10-12G_EX-10.10_6700288_EX-10.10_Co-Branding Agreement_ Agency Agreement.txt'}\n",
            "<bound method BaseModel.json of Document(page_content='CO\\n\\n\\n\\nBRANDING AND ADVERTISING AGREEMENT\\n\\nTHIS CO-BRANDING AND ADVERTISING AGREEMENT (the \"Agreement\") is made as of June 21, 1999 (the \"Effective Date\") by and between I-ESCROW, INC., with its principal place of business at 1730 S. Amphlett Blvd., Suite 233, San Mateo, California 94402 (\"i-Escrow\"), and 2THEMART.COM, INC. having its principal place of business at 18301 Von Karman Avenue, 7th Floor, Irvine, California 92612 (\"2TheMart\").\\n\\n1. DEFINITIONS.\\n\\n(a) \"CONTENT\" means all content or information, in any medium, provided by a party to the other party for use in conjunction with the performance of its obligations hereunder, including without limitation any text, music, sound, photographs, video, graphics, data or software. Content provided by 2TheMart is referred to herein as \"2TheMart Content\" and Content provided by i-Escrow is referred to herein as \"i-Escrow Content.\"\\n\\n(b) \"CO-BRANDED SITE\" means the web-site accessible through Domain Name, for the Services implemented by i-Escrow. The homepage of this web-site will visibly display both 2TheMart Marks and i-Escrow Marks.\\n\\n(c) \"CUSTOMERS\" means all users who access Co-Branded Site.\\n\\n(d) \"DOMAIN NAME\" means www.iescrow.com/2TheMart.\\n\\n(e) \"ESCROW SERVICES\" means services for auction sellers and high bidders whereby an agent holds a buyer\\'s money in trust until the buyer approves the applicable item that was physically delivered, at which time the agent releases the buyer\\'s money to seller, after subtracting the escrow fees.\\n\\n(f) \"INFORMATION TRANSFER MECHANISM\" means the mechanism by which 2TheMart transfers to i-Escrow information to populate the applicable i-Escrow transaction and user registration forms.\\n\\n(g) \"LAUNCH DATE\" means the first date on which the Co-Branded Site is pointed to in all references to i-Escrow from 2TheMart auction site, and the Information Transfer Mechanism is publicly deployed (post-beta).\\n\\n(h) \"MARKS\" means all domain names, trademarks and logos designated by a party for the other party\\'s use in conjunction with such other party\\'s performance under this Agreement. Marks designated by 2TheMart for i-Escrow\\'s use are referred to herein as \"2TheMart Marks\" and Marks designated by i-Escrow for 2TheMart\\' use are referred to herein as \"i-Escrow Marks.\"\\n\\n(i) \"SERVICES\" means i-Escrow\\'s implementation and performance of the Escrow Services as of the Effective Date, as modified over time.\\n\\n(j) \"SHADOW SITE\" means the site where Co-Branded Site is made available for 2TheMart\\'s testing of the Information Transfer Mechanism prior to being made publicly available.\\n\\n(k) \"TRANSACTION\" means a transaction utilizing the Services that actually closes and that was initiated by a Transaction Inquiry from a Customer.\\n\\n(l) \"TRANSACTION INQUIRY\" means a Customer\\'s submission of i-Escrow\\'s standard New Transaction Inquiry form (or its successor) on or through the Co-Branded Pages. Currently this means entry of a description and price of merchandise by a user (buyer or seller) who agrees to abide by the terms and conditions of the Services, together with email address of the other party, regardless of whether or not any Transaction is completed.\\n\\nSource: 2THEMART COM INC, 10-12G, 8/26/1999\\n\\n2. DEVELOPMENT AND IMPLEMENTATION.\\n\\n2.1 OVERVIEW. As set forth herein, 2TheMart will promote Services to its auction users (buyers and sellers), and i-Escrow shall develop Co-Branded Site, and develop the Information Transfer Mechanism working with 2TheMart to make Services available seamlessly to Customers. Unless otherwise specified, each party shall be responsible for all development, hosting and other costs associated with the pages resident on their servers and all emails to users they send.\\n\\n2.2 INITIAL INFORMATION TRANSFER MECHANISM DEVELOPMENT. The parties shall negotiate in good faith to determine the initial operation of the Information Transfer Mechanism and to describe such operation and development fees, in a statement of work (\"SOW\"). Each party shall make available sufficient and qualified engineers to negotiate the SOW. No SOW shall be binding on the parties unless mutually approved by both parties. In the event that the parties are unable to agree to an SOW within 2 months following the Effective Date, either party may, in its sole discretion, terminate this Agreement by providing written notice.\\n\\nOnce approved, the parties shall use commercially reasonable efforts to diligently implement their respective obligations under the SOW. Upon completion of its duties under the SOW, a party shall notify the other party and provide the other party with the opportunity to test and evaluate its work. i-Escrow shall make available the Shadow Site for such testing in a timely manner. Each party shall reasonably cooperate with the other party in effectuating their respective duties under the SOW. The Information Transfer Mechanism shall not go live until its operation has been approved (\"Approval Date\") by both parties, such approval not to be unreasonably withheld.\\n\\n2.3 LAUNCH TIMING. Each party shall use good faith and reasonable efforts to expeditiously develop the Co-Branded Pages and the Information Transfer Mechanism. In the event that, after using such efforts, the Launch Date has not occurred within 4 months following the Effective Date, either party may terminate this Agreement by providing written notice. If\\n\\nonly one party has used good faith and reasonable development efforts, only that party may exercise the foregoing right to terminate.\\n\\n2.4 RESTRICTIONS ON COMMUNICATIONS. i-Escrow may place banner advertising on the Co-Branded Site upon prior written approval of 2TheMart, which shall be at the discretion of 2TheMart. All advertising revenue arising from the banner ads shall be solely i-Escrow\\'s. i-Escrow shall not run banner advertisements on the Co-Branded Site for any of 2TheMart\\'s competitors. 2TheMart shall provide in writing, a list of companies they would like to exclude, including every time they wish to change this list.\\n\\n2.5 SERVICE PERFORMANCE OF INFORMATION TRANSFER MECHANISM. The parties each shall in good faith work to provide reasonable service levels with respect to the operation of the portions of the Information Transfer Mechanism in their control.\\n\\nSource: 2THEMART COM INC, 10-12G, 8/26/1999\\n\\n2.6 PROGRAM REVIEW MEETINGS. The parties shall meet, at least once per month either in person, or by telephone, to coordinate the implementation of this agreement over time.\\n\\n3. PROMOTION.\\n\\nAfter Launch Date, 2TheMart will widely promote the Services:\\n\\n(a) To every seller and high bidder through means including, but not limited to, end of auction emails containing links, such that, it shall be possible for the buyer or seller to initiate a Transaction Inquiry with i-Escrow, without having to re-enter all their personal or transaction related information.\\n\\n(b)  By adding links to Co-Branded Site in FAQ section of 2TheMart auctions.\\n\\n(c) By adding links to Co-Branded Site on the seller listing pages of 2TheMart auctions.\\n\\n(d) By displaying a text or graphic link to a page containing information about Services on all auction item pages and bidding pages to educate bidders about i-Escrow. 2TheMart may use the \"Escrow Services Description\" attached in Exhibit A for creating such a page.\\n\\n5. PAYMENT.\\n\\n5.1 ADVERTISING FEES. After the Launch Date, i-Escrow shall pay 2TheMart advertising fees based on the number of Transaction Inquiries. This advertising fees shall consist of a per Transaction Inquiry amount calculated by multiplying 0.025% by the amount of the average Transaction from all Customers in the preceding quarter. The formula for arriving at the per Transaction Inquiry amount may be revised from time to time during the term of this Agreement to reflect present market conditions (\"the Adjusted Rate\"), but only by mutual\\n\\nconsent of the parties after good faith discussions. The Adjusted Rate shall be added as an addendum to this Agreement.\\n\\n5.2 REPORTING. Within two (2) weeks following the end of each calendar quarter, i-Escrow shall provide to 2TheMart a report, describing for each quarter: the number of new registrations through the Co-Branded Pages; the number of Transaction Inquiries from Customers; the total number of Transactions from such inquiries; the total dollar value of the Transactions.\\n\\n5.3 AUDIT RIGHTS. i-Escrow shall keep for one (1) year proper records and books of account relating to the computation of advertising payments owed to 2TheMart (including, as appropriate, the computation of the size of average Transaction). Once every twelve (12) months, 2TheMart through a CPA may inspect and audit such records to verify reports. Any such inspection will be conducted in a manner that does not unreasonably interfere with i-Escrow\\'s business activities and with no less than fifteen (15) days notice. i-Escrow shall within two (2) weeks make any overdue payments disclosed by the audit. Such inspection shall be at 2TheMart\\'s expense; however, if the audit reveals overdue payments in excess of ten percent (10%) of the payments owed to date, i-Escrow shall immediately pay all cost of such audit.\\n\\n6. RIGHTS AND STANDARDS.\\n\\nSource: 2THEMART COM INC, 10-12G, 8/26/1999\\n\\n6.1 CONTENT. 2TheMart hereby grants to i-Escrow a worldwide, non-exclusive right to use, reproduce, distribute, publicly perform, publicly display and digitally perform the 2TheMart Content soley with respect to  and in conjunction with the Co-Branded Site all with the prior written consent of 2TheMart, for the term of this Agreement. i-Escrow hereby grants to 2TheMart a worldwide, non-exclusive right to use, reproduce, distribute, publicly perform, publicly display and digitally perform the i-Escrow Content on or in conjunction with 2TheMart auctions.\\n\\n6.2 CONTENT OWNERSHIP. Except as otherwise provided in this Agreement, as between 2TheMart and i-Escrow: (a) 2TheMart and its suppliers retain all rights, title and interest in and to all intellectual property rights embodied in or associated with the 2TheMart Content, and b) i-Escrow and its suppliers retain all rights, title and interest in and to all intellectual property rights embodied in or associated with the i-Escrow Content and Co-Branded Site. There are no implied licenses under this Agreement, and any rights not expressly granted are reserved. Neither party shall exceed the scope of the rights granted hereunder.\\n\\n6.3 TRADEMARKS. Subject to the terms and conditions of this Agreement: (a) i-Escrow hereby grants to 2TheMart a non-exclusive, nontransferable right to use the i-Escrow Marks (including without limitation the Domain Name) in links to and advertisements and promotions for the Co-Branded Pages or the Services; and (b) 2TheMart hereby grants to i-Escrow a non-exclusive, nontransferable right to use 2TheMart Marks (including without limitation the Domain Name) on the Co-Branded Pages, and for the performance of  Services.\\n\\n6.4 TRADEMARK RESTRICTIONS. The Mark owner may terminate the foregoing rights if, in its reasonable discretion, the other party\\'s use of the Marks tarnishes, blurs or dilutes the quality associated with the Marks or the associated goodwill and such problem is not cured within ten (10) days of notice of breach; alternatively, instead of terminating the right in total, the\\n\\nowner may specify that certain pages of the other party\\'s web-site may not contain the Marks. Title to and ownership of the owner\\'s Marks shall remain with the owner. The receiving party shall use the Marks exactly in the form provided and in conformance with any trademark usage policies. The other party shall not take any action inconsistent with the owner\\'s ownership of the Marks, and any benefits accruing from use of such Marks shall automatically vest in the owner. The other party shall not form any combination marks with the other party\\'s Marks. Notwithstanding the foregoing, to the extent that the Domain Name is deemed a combination mark, neither party shall use the Domain Name for any purpose except as expressly provided herein or attempt to register the Domain Name, and the parties will jointly cooperate on any enforcement action of infringement of the Domain Name.\\n\\n6.5 LIMITS ON SUBLICENSING. All rights (under any applicable intellectual property right) granted herein are not sublicenseable,\\n\\nSource: 2THEMART COM INC, 10-12G, 8/26/1999\\n\\ntransferable or assignable. Notwithstanding the foregoing, either party may use a third party web host, but all actions or failures to act of the web host that would be a breach of this Agreement, were the actions or failures to act taken by the applicable party, shall be deemed a breach of this Agreement. In addition, 2TheMart may grant sublicenses to companies that 2TheMart has a business relationship with to the extent that 2TheMart Content is visible from such company\\'s web-site through a link or other means.\\n\\n6.6 CONTENT STANDARDS. 2TheMart shall not provide any 2TheMart Content, and i-Escrow shall not provide any i-Escrow Content, that: (a) infringes any third party\\'s copyright, patent, trademark, trade secret or other proprietary rights or rights of publicity or privacy; (b) violates any law, statute, ordinance or regulation (including without limitation the laws and regulations governing export control, unfair competition, antidiscrimination or false advertising); (c) is defamatory, trade libelous, unlawfully threatening or unlawfully harassing; (d) is obscene, harmful to minors or child pornographic; (e) contains any viruses, Trojan horses, worms, time bombs, cancelbots or other computer programming routines that are intended to damage, detrimentally interfere with, surreptitiously intercept or expropriate any system, data or personal information; and (f) is materially false, misleading or inaccurate.\\n\\n6.7  SERVICE STANDARDS. i-Escrow will comply with all laws and regulations and act as an Independent Escrow Agent as per the guidelines of California Escrow Law (California Financial Code Section17000 et seq., or its successor). Should any of the terms, conditions or provisions of this Agreement conflict with the California Escrow Law, its rules or regulations, which govern i-Escrow\\'s business practices, the California Escrow Law shall prevail. Notwithstanding the foregoing, at any time that i-Escrow reasonably believes such a conflict exists, i-Escrow will give 2TheMart written notice of such conflict and the parties will use their best efforts to resolve such conflict.\\n\\n7. DISCLAIMER OF WARRANTIES. EACH PARTY PROVIDES ALL MATERIALS AND SERVICES TO THE OTHER PARTY \"AS IS.\" EACH PARTY DISCLAIMS ALL WARRANTIES AND CONDITIONS, EXPRESS, IMPLIED OR STATUTORY, INCLUDING WITHOUT LIMITATION THE IMPLIED WARRANTIES OF TITLE, NON-\\n\\nINFRINGEMENT, MERCHANTABILITY ANDFITNESS FOR A PARTICULAR PURPOSE. Each party acknowledges that it has not entered into this Agreement in reliance upon any warranty or representation except those specifically set forth herein.\\n\\n8. TERM AND TERMINATION.\\n\\n8.1 TERM. The term of this Agreement shall continue for one (1) year following the Launch Date, unless earlier terminated as provided herein. This Agreement may be renewed for any number of successive one (1) year terms by mutual written agreement of the parties prior to the conclusion of the term of this Agreement. A party wishing to renew this Agreement shall give the other party notice thereof no less than thirty (30) days before the expiration of the term then in effect. In the event that either party does not give such notice, the term of this Agreement shall be automatically renewed for another one (1) year.\\n\\nSource: 2THEMART COM INC, 10-12G, 8/26/1999\\n\\n8.2 TERMINATION FOR BREACH. In addition to other remedies that may be available to it, by providing written notice, a party may immediately terminate this Agreement: (a) if the other party materially breaches this Agreement and fails to cure that breach within sixty (60) days after receiving written notice of the breach, or (b) as provided in Sections 2.2  [INITIAL INFORMATION TRANSFER MECHANISM DEVELOPMENT], 2.4  [RESTRICTIONS ON COMMUNICATIONS], or 12.4.\\n\\n8.3 TERMINATION FOR CHANGE IN COMPANY STRUCTURE. If a majority of the equity securities of either 2TheMart or i-Escrow, Inc. (except that i-Escrow may sell all or a majority of its equity securities or voting interests to i-Escrow.com, and i-Escrow.com may sell all or a majority of its equity securities or voting interests to i-Escrow\\'s existing shareholders, without triggering the foregoing) are acquired by another company during the term of this Agreement either company may terminate this Agreement, without liability, by giving a thirty (30) days written notice to the other party.\\n\\n8.4 TERMINATION FOR BANKRUPTCY. Either party may terminate or suspend this Agreement effective immediately and without liability upon written notice to the other party if any one of the following events occurs:\\n\\n(a) the other party files a voluntary petition in bankruptcy or otherwise seeks protection under any law for the protection of debtors;\\n\\n(b) a proceeding is instituted against the other party under any provision of any bankruptcy laws which is not dismissed within ninety (90) days;\\n\\n(c) the other party is adjudged  bankrupt;\\n\\n(d) a court assumes jurisdiction of all or a substantial portion of the assets of the other party under a reorganization law;\\n\\n(e) a trustee or receiver is appointed by a court for all or a substantial portion of the assets of the other party;\\n\\n(f) the other party becomes insolvent, ceases or suspends all or substantially all of its business; or\\n\\n(g) the other party makes an assignment of the majority of its assets for the benefit of its creditors.\\n\\n8.5 EFFECTS OF TERMINATION. Upon expiration or termination of this Agreement for any reason: (a) all rights granted herein shall terminate, (b) i-Escrow shall pay all amounts owed to 2TheMart within six (6) weeks of termination, and (c) each party shall remove the other party\\'s content and Marks from their servers. Notwithstanding the foregoing, unless this Agreement was terminated for a material breach, all provisions of this Agreement shall survive to the extent necessary for i-Escrow to complete any Customer transactions which are pending at the time of expiration or termination. Sections 1, 7,  8.5  [EFFECTS OF TERMINATION], 9, 10, 11 and 12 shall survive expiration or termination of this Agreement.\\n\\n9. INDEMNITY. Each party (the \"Indemnifying Party\") shall indemnify the other party (the \"Indemnified Party\") against any and all claims, losses, costs and expenses, including reasonable attorneys\\' fees, which the Indemnified Party may incur as a result of claims in any form by third parties arising from the Indemnifying Party\\'s acts, omissions or misrepresentations to the extent that the Indemnified Party is deemed a principal of the Indemnifying Party. In addition, 2TheMart shall indemnify i-Escrow against any and all claims, losses, costs and expenses, including reasonable attorneys\\' fees, which i-Escrow may incur as a result of claims in any form by third parties arising from 2TheMart Content. In addition, i-Escrow shall indemnify 2TheMart against any and all claims, losses, costs and expenses, including reasonable attorneys\\' fees, which 2TheMart may incur as a result of claims in any form by third parties arising from i-Escrow\\n\\nSource: 2THEMART COM INC, 10-12G, 8/26/1999\\n\\nContent and or the Services provided to Customers. The foregoing obligations are conditioned on the Indemnified Party: (i) giving the Indemnifying Party notice of the relevant claim, (ii) cooperating with the Indemnifying Party, at the Indemnifying Party\\'s expense, in the defense of such claim, and (iii) giving the Indemnifying Party the right to control the defense and settlement of any such claim, except that the Indemnifying Party shall not enter into any settlement that affects the Indemnified Party\\'s rights or interest without the Indemnified Party\\'s prior written approval. The Indemnified Party shall have the right to participate in the defense at its expense.\\n\\n10. LIMITATION ON LIABILITY. EXCEPT IN THE EVENT OF A BREACH OF SECTION 11, NEITHER PARTY SHALL BE LIABLE FOR SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES OR LOST PROFITS (HOWEVER ARISING, INCLUDING NEGLIGENCE) ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT, EVEN IF THE PARTIES ARE AWARE OF THE POSSIBILITY OF SUCH DAMAGES.\\n\\n11. CONFIDENTIAL INFORMATION. A party\\'s \"Confidential Information\" is defined as any confidential or proprietary information of a party which is disclosed to the other party in a writing marked confidential or, if disclosed orally, is identified as confidential at the time of disclosure and is subsequently reduced to a writing marked confidential and delivered to the\\n\\nother party within ten (10) days of disclosure. Each party shall hold the other party\\'s Confidential Information in confidence and shall not disclose such Confidential Information to third parties nor use the other party\\'s Confidential Information for any purpose other than as required to perform under this Agreement. Such restrictions shall not apply to Confidential Information which (a) is already known by the recipient, (b) becomes, through no act or fault of the recipient, publicly known, (c) is received by recipient from a third party without a restriction on disclosure or use, or (d) is independently developed by recipient without reference to the Confidential Information. The restriction on disclosure shall not apply to Confidential Information which is required to be disclosed by a court or government agency. Upon expiration or termination of this Agreement, within fourteen (14) days of the other party\\'s request, each party will return all Confidential Information and other deliverables to the requesting party.\\n\\n12. GENERAL PROVISIONS.\\n\\n12.1 GOVERNING LAW. This Agreement will be governed and construed in accordance with the laws of the State of California without giving effect to conflict of laws principles. Both parties submit to personal jurisdiction in California and further agree that any cause of action arising under this Agreement shall be brought in a court in Orange County, California.\\n\\n12.2 SEVERABILITY; HEADINGS. If any provision herein is held to be invalid or unenforceable for any reason, the remaining provisions will continue in full force without being impaired or invalidated in any way. The parties agree to replace any invalid provision with a valid provision that most closely approximates the intent and economic effect of the invalid provision. Headings are for reference purposes only and in no way define, limit, construe or describe the scope or extent of such section.\\n\\n12.3 PUBLICITY. Prior to the release of any press releases or other similar promotional materials related to this Agreement, the releasing party shall submit a written request for approval to the other party with a copy of the materials to be released, which\\n\\nSource: 2THEMART COM INC, 10-12G, 8/26/1999\\n\\nrequest shall be made no less than three (3) business days prior to the requested release date. A party shall not unreasonably withhold or delay the granting of its approval of such materials, and such approval shall be provided to the other party within one (1) business day of receipt\\n\\n12.4 FORCE MAJEURE. Except as otherwise provided, if performance hereunder (other than payment) is prevented, restricted or interfered with by any act or condition whatsoever beyond the reasonable control of a party (a \"force majeure event\"), the party so affected, upon giving prompt notice to the other party, shall be excused from such performance to the extent of such prevention, restriction or interference. However, if a force majeure event interferes with the operation of this Agreement for sixty (60) days or more, either party can terminate this Agreement, without penalty. Notwithstanding the foregoing, the occurrence of any force majeure event shall not limit either party\\'s obligations under Section 9 with respect to any third party claim as to which the other party seeks indemnification.\\n\\n12.5 INDEPENDENT CONTRACTORS. The parties are independent contractors, and no agency, partnership, joint venture, employee- employer or franchisor-franchisee relationship is\\n\\nintended or created by this Agreement. Neither party shall make any warranties or representations on behalf of the other party.\\n\\n12.6 NOTICE. Any notices hereunder shall be given to the appropriate party at the address specified below or at such other address as the party shall specify in writing. Notice shall be deemed given: upon personal delivery; if sent by fax, upon confirmation of receipt; or if sent by a reputable overnight courier with tracking capabilities, one (1) day after the date of mailing: To i-Escrow:            i-Escrow, Inc.                         1730 South Amphlett Blvd., #215                         San Mateo, CA 94402                         Fax no. (650) 638-7890                         Attention:  President\\n\\nWith copy to:           Fred M. Greguras, Esq. Legal Counsel of i-Escrow                         Fenwick & West LLP                         Two Palo Alto Square                         Palo Alto, CA 94306\\n\\nTo 2TheMart:            Dominic J. Magliarditi                         President                         18301 Von Karman Avenue,                         7th Floor                         Irvine, CA 92612                         Fax no. (949) 477-1221\\n\\n11.7 COUNTERPARTS. This Agreement may be executed in one or more counterparts, each of which shall be deemed an original and all of which shall be taken together and deemed to be one instrument.\\n\\n12.8 GOOD FAITH. The parties agree to act in good faith with respect to each provision of this Agreement and any dispute that may arise related hereto.\\n\\n12.9 ADDITIONAL DOCUMENTS/INFORMATION. The parties agree to sign and/or provide such additional documents and/or information as may reasonably be required to carry out the intent of this Agreement and to effectuate its purposes.\\n\\n12.10 RIGHTS AND REMEDIES CUMULATIVE. The rights and remedies provided herein will be cumulative and not exclusive of any other rights or remedies provided by law or otherwise.\\n\\n12.11 NONWAIVER. No failure or forbearance by either party to exercise any right or insist upon or enforce performance of any obligation hereunder shall be deemed a waiver or relinquishment to any extent of that or any other right or obligation, in that or any other instance; rather, the\\n\\nSource: 2THEMART COM INC, 10-12G, 8/26/1999\\n\\nsame shall be and shall remain in full force and effect. Any waiver of any right of a party or any obligation of the other party hereunder must be made in a writing signed by the arty waiving such right or obligation.\\n\\n12.12 ENTIRE AGREEMENT. This Agreement contains the entire understanding of the parties hereto with respect to the transactions and matters contemplated hereby, supersedes all previous Agreements between i-Escrow and 2TheMart concerning the subject matter (except for the Confidential Agreement Dated January 4 1999, which shall survive this Agreement). No amendments or supplements to this Agreement will be effective for any purpose except by a written Agreement signed by the parties. No party hereto has relied on any statement, representation or promise of any party or with any other officer, agent, employee or attorney for the other party in executing this Agreement except as expressly stated herein.\\n\\n2THEMART.COM, INC.:                         I-ESCROW, INC.:\\n\\nBy:/s/Dominic J. Magliarditi                By:/s/Sanjay Bajaj Name: Dominic J. Magliarditi                Name: Sanjay Bajaj Title: President                            Title: VP Business Development Date: 6/21/99                               Date: 6/11/99\\n\\nEXHIBIT A\\n\\nESCROW SERVICES DESCRIPTION\\n\\nSuccessful completion of a transaction involves exchange of merchandise with payment. The buyer has to be satisfied he/she received what they thought they were getting and the seller has to be sure he/she gets paid. i-Escrow holds payment from the buyer in trust until the seller sends the merchandise to the buyer. Once the buyer accepts the merchandise, i-Escrow forwards the payment to the seller by writing a check. A typical escrow transaction: When an auction ends, your end of auction email contains links to i-Escrow. Once you have signed up with i-Escrow   you go through the following steps to complete your transaction. 1. Start a transaction by entering the description and price of the merchandise along with email address of the other party. 2. The other party receives an email from i-Escrow requesting an acknowledgement of the terms of the transaction. 3. Once the transaction is acknowledged by the other party, the buyer pays i-Escrow the agreed upon price, by credit card or other means. 4.   i-Escrow informs the seller that payment has been received, requesting them to ship the merchandise directly to the buyer. 5. The seller provides i-Escrow with the tracking number of the shipment. 6. The buyer receives and accepts the merchandise. 7.   i-Escrow sends the check to the seller.\\n\\nFor more information about I-Escrow, visit their web-site at www.iescrow.com\\n\\nSource: 2THEMART COM INC, 10-12G, 8/26/1999', metadata={'source': 'gs://contractunderstandingatticusdataset/2ThemartComInc_19990826_10-12G_EX-10.10_6700288_EX-10.10_Co-Branding Agreement_ Agency Agreement.txt'})>\n"
          ]
        }
      ],
      "source": [
        "print(f\"# of documents = {len(contractdocs)}\")\n",
        "print(contractdocs[0].metadata)\n",
        "print( contractdocs[0].json)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Da7_1bpFGpb"
      },
      "source": [
        "*Split documents into chunks as needed by the token limit of the LLM and let there be an overlap between the chunks*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2E_qzSuMFHKt",
        "outputId": "fe06e73a-d534-48cf-d685-d7b9e345a4af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of document splits = 2150\n"
          ]
        }
      ],
      "source": [
        "# split the documents into chunks\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "doc_splits = text_splitter.split_documents(contractdocs)\n",
        "print(f\"# of document splits = {len(doc_splits)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Capture Metadata*"
      ],
      "metadata": {
        "id": "lUG4baXr8QhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add chunk number to metadata\n",
        "for idx, split in enumerate(doc_splits):\n",
        "    #print(idx)\n",
        "    split.metadata[\"chunk\"] = idx\n",
        "\n",
        "# Separate doc_splits to semantic data and meta data\n",
        "texts = [doc.page_content for doc in doc_splits]\n",
        "metadatas = [\n",
        "    [\n",
        "        {\"namespace\": \"source\", \"allow_list\": [doc.metadata[\"source\"]]},\n",
        "        {\"namespace\": \"document_name\", \"allow_list\": [doc.metadata[\"source\"].split(\"/\")[-1]]},\n",
        "        {\"namespace\": \"chunk\", \"allow_list\": [str(doc.metadata[\"chunk\"])]},\n",
        "    ]\n",
        "    for doc in doc_splits\n",
        "]\n",
        "print(f\"# of document splits = {len(doc_splits)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxwvbX3227xm",
        "outputId": "2a93aa59-fa6a-45ae-bb0b-a7226a6b1ff6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of document splits = 2150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metadatas[3]"
      ],
      "metadata": {
        "id": "64kH7M1XF6Nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load semantic data as texts and metadata into Matching Engine\n",
        "This takes time: 7mins ++"
      ],
      "metadata": {
        "id": "8I7s_89e8xI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_ids = me.add_texts(texts=texts, metadatas=metadatas)"
      ],
      "metadata": {
        "id": "jjMsKR6t2lU9"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*Do some testing of Matching Engine*"
      ],
      "metadata": {
        "id": "6-RAqBDlHHrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test whether search from vector store is working\n",
        "me.similarity_search(\"image\", k=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHqWi45k1cZ4",
        "outputId": "36da5a42-dcbc-4723-b42f-a9917ce1af3b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='th', metadata={'source': 'gs://contractunderstandingatticusdataset/ASPIRITYHOLDINGSLLC_05_07_2012-EX-10.6-OUTSOURCING AGREEMENT.txt', 'document_name': 'ASPIRITYHOLDINGSLLC_05_07_2012-EX-10.6-OUTSOURCING AGREEMENT.txt', 'chunk': '1735', 'score': 0.6547313332557678}),\n",
              " Document(page_content='th', metadata={'source': 'gs://contractunderstandingatticusdataset/ASPIRITYHOLDINGSLLC_05_07_2012-EX-10.6-OUTSOURCING AGREEMENT.txt', 'document_name': 'ASPIRITYHOLDINGSLLC_05_07_2012-EX-10.6-OUTSOURCING AGREEMENT.txt', 'chunk': '1735', 'score': 0.6547313332557678})]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "me.similarity_search(\"Twin Cities Power Holdings\", k=2, search_distance=0.4)\n"
      ],
      "metadata": {
        "id": "yj8sWZFq9smL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVCftNbU-wp0"
      },
      "source": [
        "## Obtain handle to the retriever\n",
        "\n",
        "We will use the native retriever provided by Chroma DB to perform similarity search within the contracts document vector store among the different document chunks so as to return that document chunk which has the lowest vectoral \"distance\" with the incoming user query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Xb2VCn6e-0zp"
      },
      "outputs": [],
      "source": [
        "# Retriever configuration\n",
        "NUMBER_OF_RESULTS = 10\n",
        "SEARCH_DISTANCE_THRESHOLD = 0.6\n",
        "\n",
        "# Expose index to the retriever\n",
        "retriever = me.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\n",
        "        \"k\": NUMBER_OF_RESULTS,\n",
        "        \"search_distance\": SEARCH_DISTANCE_THRESHOLD,\n",
        "    },\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcwQNvfN_6Rn"
      },
      "source": [
        "## Define a Retrieval QA Chain to use retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ElWUO3fQAMaH"
      },
      "outputs": [],
      "source": [
        "# Create chain to answer questions\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "llm = VertexLLM(\n",
        "    model_name='text-bison-32k',\n",
        "    max_output_tokens=256,\n",
        "    temperature=0.1,\n",
        "    top_p=0.8,\n",
        "    top_k=40,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# Uses LLM to synthesize results from the search index.\n",
        "# We use Vertex PaLM Text API for LLM\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHYlgYQhFTQW"
      },
      "source": [
        "## Leverage LLM to search from retriever"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUZDkQOrGb9X"
      },
      "source": [
        "*Example:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGTS8x7TFoOn",
        "outputId": "17b192ab-7950-4e2e-dcfe-3060cbab5084"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'Who all entered into agreement with Sagebrush?', 'result': ' The Partnership, all of the Partners except Alpha Mariah (Prime), Inc. and Beta Mariah (Prime) Inc., and Manager entered into that certain Sagebrush Management and Maintenance Agreement, dated as of September 1, 1989 (the \"Agreement\").', 'source_documents': [Document(page_content='Each party cooperated and participated in the drafting and preparation of this Agreement and the documents referred to herein, and any and all drafts relating thereto exchanged among the parties shall be deemed the work product of all of the parties and may not be construed against any party by reason of its drafting or preparation. Accordingly, any rule of law or any legal decision that would require interpretation of any ambiguities in this Agreement against any party that drafted or prepared it is of no application and is hereby expressly waived by each of the parties hereto, and any controversy over interpretations of this Agreement shall be decided without regards to events of drafting or preparation.\\n\\n[Signature Pages Follow]   7\\n\\nIN WITNESS WHEREOF, each of the parties hereto has executed this COOPERATION AGREEMENT or caused the same to be executed by its duly authorized representative as of the date first above written. Allison Transmission Holdings, Inc.', metadata={'source': 'gs://contractunderstandingatticusdataset/ALLISONTRANSMISSIONHOLDINGSINC_12_15_2014-EX-99.1-COOPERATION AGREEMENT.txt', 'document_name': 'ALLISONTRANSMISSIONHOLDINGSINC_12_15_2014-EX-99.1-COOPERATION AGREEMENT.txt', 'chunk': '1346', 'score': 0.70986008644104}), Document(page_content='Each party cooperated and participated in the drafting and preparation of this Agreement and the documents referred to herein, and any and all drafts relating thereto exchanged among the parties shall be deemed the work product of all of the parties and may not be construed against any party by reason of its drafting or preparation. Accordingly, any rule of law or any legal decision that would require interpretation of any ambiguities in this Agreement against any party that drafted or prepared it is of no application and is hereby expressly waived by each of the parties hereto, and any controversy over interpretations of this Agreement shall be decided without regards to events of drafting or preparation.\\n\\n[Signature Pages Follow]   7\\n\\nIN WITNESS WHEREOF, each of the parties hereto has executed this COOPERATION AGREEMENT or caused the same to be executed by its duly authorized representative as of the date first above written. Allison Transmission Holdings, Inc.', metadata={'source': 'gs://contractunderstandingatticusdataset/ALLISONTRANSMISSIONHOLDINGSINC_12_15_2014-EX-99.1-COOPERATION AGREEMENT.txt', 'document_name': 'ALLISONTRANSMISSIONHOLDINGSINC_12_15_2014-EX-99.1-COOPERATION AGREEMENT.txt', 'chunk': '1346', 'score': 0.709692120552063}), Document(page_content='Each party cooperated and participated in the drafting and preparation of this Agreement and the documents referred to herein, and any and all drafts relating thereto exchanged among the parties shall be deemed the work product of all of the parties and may not be construed against any party by reason of its drafting or preparation. Accordingly, any rule of law or any legal decision that would require interpretation of any ambiguities in this Agreement against any party that drafted or prepared it is of no application and is hereby expressly waived by each of the parties hereto, and any controversy over interpretations of this Agreement shall be decided without regards to events of drafting or preparation.\\n\\n[Signature Pages Follow]   7\\n\\nIN WITNESS WHEREOF, each of the parties hereto has executed this COOPERATION AGREEMENT or caused the same to be executed by its duly authorized representative as of the date first above written. Allison Transmission Holdings, Inc.', metadata={'source': 'gs://contractunderstandingatticusdataset/ALLISONTRANSMISSIONHOLDINGSINC_12_15_2014-EX-99.1-COOPERATION AGREEMENT.txt', 'document_name': 'ALLISONTRANSMISSIONHOLDINGSINC_12_15_2014-EX-99.1-COOPERATION AGREEMENT.txt', 'chunk': '1346', 'score': 0.709692120552063}), Document(page_content='Each party cooperated and participated in the drafting and preparation of this Agreement and the documents referred to herein, and any and all drafts relating thereto exchanged among the parties shall be deemed the work product of all of the parties and may not be construed against any party by reason of its drafting or preparation. Accordingly, any rule of law or any legal decision that would require interpretation of any ambiguities in this Agreement against any party that drafted or prepared it is of no application and is hereby expressly waived by each of the parties hereto, and any controversy over interpretations of this Agreement shall be decided without regards to events of drafting or preparation.\\n\\n[Signature Pages Follow]   7\\n\\nIN WITNESS WHEREOF, each of the parties hereto has executed this COOPERATION AGREEMENT or caused the same to be executed by its duly authorized representative as of the date first above written. Allison Transmission Holdings, Inc.', metadata={'source': 'gs://contractunderstandingatticusdataset/ALLISONTRANSMISSIONHOLDINGSINC_12_15_2014-EX-99.1-COOPERATION AGREEMENT.txt', 'document_name': 'ALLISONTRANSMISSIONHOLDINGSINC_12_15_2014-EX-99.1-COOPERATION AGREEMENT.txt', 'chunk': '1346', 'score': 0.709692120552063}), Document(page_content='Exhibit 10.17(b)                                                                 ----------------\\n\\nFIRST AMENDMENT TO SAGEBRUSH                       MANAGEMENT AND MAINTENANCE AGREEMENT\\n\\nTHIS FIRST AMENDMENT TO SAGEBRUSH MANAGEMENT AND MAINTENANCE AGREEMENT (\"Amendment\") is made and entered into as of December 1, 1990 by and among Sagebrush, a California general partnership (\"Partnership\"), the undersigned partners of the Partnership, being all of the Sagebrush partners (\"Partners\"), and ToyoWest Management Inc., a California corporation (\"Manager\"), with respect to the following facts and circumstances:\\n\\nR E C I T A L S                                  ---------------\\n\\nA. The Partnership, all of the Partners except Alpha Mariah (Prime), Inc. and Beta Mariah (Prime) Inc., and Manager entered into that certain Sagebrush Management and Maintenance Agreement, dated as of September 1, 1989 (the \"Agreement\").', metadata={'source': 'gs://contractunderstandingatticusdataset/ZONDWINDSYSTEMPARTNERSLTDSERIES85-B_04_03_2006-EX-10-MANAGEMENT AND MAINTENANCE AGREEMENT.txt', 'document_name': 'ZONDWINDSYSTEMPARTNERSLTDSERIES85-B_04_03_2006-EX-10-MANAGEMENT AND MAINTENANCE AGREEMENT.txt', 'chunk': '2130', 'score': 0.6899971961975098}), Document(page_content='Exhibit 10.17(b)                                                                 ----------------\\n\\nFIRST AMENDMENT TO SAGEBRUSH                       MANAGEMENT AND MAINTENANCE AGREEMENT\\n\\nTHIS FIRST AMENDMENT TO SAGEBRUSH MANAGEMENT AND MAINTENANCE AGREEMENT (\"Amendment\") is made and entered into as of December 1, 1990 by and among Sagebrush, a California general partnership (\"Partnership\"), the undersigned partners of the Partnership, being all of the Sagebrush partners (\"Partners\"), and ToyoWest Management Inc., a California corporation (\"Manager\"), with respect to the following facts and circumstances:\\n\\nR E C I T A L S                                  ---------------\\n\\nA. The Partnership, all of the Partners except Alpha Mariah (Prime), Inc. and Beta Mariah (Prime) Inc., and Manager entered into that certain Sagebrush Management and Maintenance Agreement, dated as of September 1, 1989 (the \"Agreement\").', metadata={'source': 'gs://contractunderstandingatticusdataset/ZONDWINDSYSTEMPARTNERSLTDSERIES85-B_04_03_2006-EX-10-MANAGEMENT AND MAINTENANCE AGREEMENT.txt', 'document_name': 'ZONDWINDSYSTEMPARTNERSLTDSERIES85-B_04_03_2006-EX-10-MANAGEMENT AND MAINTENANCE AGREEMENT.txt', 'chunk': '2130', 'score': 0.6899971961975098}), Document(page_content='Exhibit 10.17(b)                                                                 ----------------\\n\\nFIRST AMENDMENT TO SAGEBRUSH                       MANAGEMENT AND MAINTENANCE AGREEMENT\\n\\nTHIS FIRST AMENDMENT TO SAGEBRUSH MANAGEMENT AND MAINTENANCE AGREEMENT (\"Amendment\") is made and entered into as of December 1, 1990 by and among Sagebrush, a California general partnership (\"Partnership\"), the undersigned partners of the Partnership, being all of the Sagebrush partners (\"Partners\"), and ToyoWest Management Inc., a California corporation (\"Manager\"), with respect to the following facts and circumstances:\\n\\nR E C I T A L S                                  ---------------\\n\\nA. The Partnership, all of the Partners except Alpha Mariah (Prime), Inc. and Beta Mariah (Prime) Inc., and Manager entered into that certain Sagebrush Management and Maintenance Agreement, dated as of September 1, 1989 (the \"Agreement\").', metadata={'source': 'gs://contractunderstandingatticusdataset/ZONDWINDSYSTEMPARTNERSLTDSERIES85-B_04_03_2006-EX-10-MANAGEMENT AND MAINTENANCE AGREEMENT.txt', 'document_name': 'ZONDWINDSYSTEMPARTNERSLTDSERIES85-B_04_03_2006-EX-10-MANAGEMENT AND MAINTENANCE AGREEMENT.txt', 'chunk': '2130', 'score': 0.6899971961975098}), Document(page_content='Exhibit 10.17(b)                                                                 ----------------\\n\\nFIRST AMENDMENT TO SAGEBRUSH                       MANAGEMENT AND MAINTENANCE AGREEMENT\\n\\nTHIS FIRST AMENDMENT TO SAGEBRUSH MANAGEMENT AND MAINTENANCE AGREEMENT (\"Amendment\") is made and entered into as of December 1, 1990 by and among Sagebrush, a California general partnership (\"Partnership\"), the undersigned partners of the Partnership, being all of the Sagebrush partners (\"Partners\"), and ToyoWest Management Inc., a California corporation (\"Manager\"), with respect to the following facts and circumstances:\\n\\nR E C I T A L S                                  ---------------\\n\\nA. The Partnership, all of the Partners except Alpha Mariah (Prime), Inc. and Beta Mariah (Prime) Inc., and Manager entered into that certain Sagebrush Management and Maintenance Agreement, dated as of September 1, 1989 (the \"Agreement\").', metadata={'source': 'gs://contractunderstandingatticusdataset/ZONDWINDSYSTEMPARTNERSLTDSERIES85-B_04_03_2006-EX-10-MANAGEMENT AND MAINTENANCE AGREEMENT.txt', 'document_name': 'ZONDWINDSYSTEMPARTNERSLTDSERIES85-B_04_03_2006-EX-10-MANAGEMENT AND MAINTENANCE AGREEMENT.txt', 'chunk': '2130', 'score': 0.6899837851524353}), Document(page_content=\"32\\n\\nIf the foregoing correctly sets forth the arrangement among the Company, the Bank and the Agent, please indicate acceptance thereof in the space  provided below for that purpose, whereupon this letter and the Agent's acceptance shall constitute a binding agreement.\\n\\nAccepted as of the date first above written\\n\\n33\\n\\nVery truly yours,                   ATHENS FEDERAL COMMUNITY BANK        ATHENS BANCSHARES CORPORATION           By Its Authorized Representative        By Its Authorized Representative:\\n\\nJeff Cunningham        Jeff Cunningham President and Chief Executive Officer        President and Chief Executive Officer\\n\\nKEEFE, BRUYETTE & WOODS, INC.    By its Authorized Representative\\n\\nHarold T. Hanley, III, Managing Director     Managing Director\", metadata={'source': 'gs://contractunderstandingatticusdataset/ATHENSBANCSHARESCORP_11_02_2009-EX-1.2-AGENCY AGREEMENT , 2009.txt', 'document_name': 'ATHENSBANCSHARESCORP_11_02_2009-EX-1.2-AGENCY AGREEMENT , 2009.txt', 'chunk': '2049', 'score': 0.6725590229034424}), Document(page_content=\"32\\n\\nIf the foregoing correctly sets forth the arrangement among the Company, the Bank and the Agent, please indicate acceptance thereof in the space  provided below for that purpose, whereupon this letter and the Agent's acceptance shall constitute a binding agreement.\\n\\nAccepted as of the date first above written\\n\\n33\\n\\nVery truly yours,                   ATHENS FEDERAL COMMUNITY BANK        ATHENS BANCSHARES CORPORATION           By Its Authorized Representative        By Its Authorized Representative:\\n\\nJeff Cunningham        Jeff Cunningham President and Chief Executive Officer        President and Chief Executive Officer\\n\\nKEEFE, BRUYETTE & WOODS, INC.    By its Authorized Representative\\n\\nHarold T. Hanley, III, Managing Director     Managing Director\", metadata={'source': 'gs://contractunderstandingatticusdataset/ATHENSBANCSHARESCORP_11_02_2009-EX-1.2-AGENCY AGREEMENT , 2009.txt', 'document_name': 'ATHENSBANCSHARESCORP_11_02_2009-EX-1.2-AGENCY AGREEMENT , 2009.txt', 'chunk': '2049', 'score': 0.6724700927734375})]}\n"
          ]
        }
      ],
      "source": [
        "query = \"Who all entered into agreement with Sagebrush?\"\n",
        "result = qa({\"query\": query})\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgYa4k1WiolM"
      },
      "source": [
        "## Build a Front End"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRjBQsXqirbC"
      },
      "outputs": [],
      "source": [
        "!pip install -q gradio\n",
        "import gradio as gr\n",
        "import markdown\n",
        "\n",
        "def chatbot(inputtext):\n",
        "    result = qa({\"query\": inputtext})\n",
        "\n",
        "    return result['result'], get_public_url(result['source_documents'][0].metadata['source']), result['source_documents'][0].metadata['source']\n",
        "\n",
        "from google.cloud import storage\n",
        "\n",
        "def get_public_url(uri):\n",
        "    \"\"\"Returns the public URL for a file in Google Cloud Storage.\"\"\"\n",
        "    # Split the URI into its components\n",
        "    components = uri.split(\"/\")\n",
        "\n",
        "    # Get the bucket name\n",
        "    bucket_name = components[2]\n",
        "\n",
        "    # Get the file name\n",
        "    file_name = components[3]\n",
        "\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(file_name)\n",
        "    return blob.public_url\n",
        "\n",
        "\n",
        "print(\"Launching Gradio\")\n",
        "\n",
        "iface = gr.Interface(fn=chatbot,\n",
        "                     inputs=[gr.Textbox(label=\"Query\")],\n",
        "                     examples=[\"What is the agreement made by Twin Cities Power Holdings\", \"What is the agreement between MICOA & Stratton Cheeseman\", \"What is the commission % that Stratton Cheeseman will get from MICOA and how much will they get if MICOA's revenues are $100\"],\n",
        "                     title=\"Contract Analyst\",\n",
        "                     outputs=[gr.Textbox(label=\"Response\"),\n",
        "                              gr.Textbox(label=\"URL\"),\n",
        "                              gr.Textbox(label=\"Cloud Storage URI\")],\n",
        "                     theme=gr.themes.Soft)\n",
        "\n",
        "iface.launch(share=False)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}